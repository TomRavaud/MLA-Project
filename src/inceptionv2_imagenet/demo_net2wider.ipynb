{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Demo:** Net2WiderNet on ImageNet with Inception-V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demo shows how to apply Net2WiderNet to Inception-V2 in order to increase the number of output filters in each layer of the Inception blocks. The input image shape is the one of ImageNet, but the network and the Net2WiderNet algorithm can be applied to any other image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchinfo\n",
    "\n",
    "# Import custom modules and packages\n",
    "from inceptionv2 import GoogleNetBN\n",
    "import params.inceptionv2_imagenet\n",
    "import net2net.net2net_wider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create an Inception-V2 model narrower than the original one\n",
    "\n",
    "We start by creating an Inception-V2 model, narrower than the standard model: the number of convolution channels at each layer within all Inception modules is reduced by a factor of $\\sqrt{0.3}$. The rest of the network remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a downsized version of the Inception-V2 network\n",
    "# (with 10 classes instead of 1000 for demo purposes)\n",
    "model = GoogleNetBN(nb_classes=10, inception_factor=np.sqrt(0.3))\n",
    "\n",
    "# Create a random input\n",
    "x = torch.randn(1,\n",
    "                params.inceptionv2_imagenet.NB_CHANNELS,\n",
    "                *params.inceptionv2_imagenet.IMAGE_SHAPE)\n",
    "\n",
    "# Compute the output of the teacher network\n",
    "# (forward pass to initialize the Lazy modules)\n",
    "y_teacher = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recover the standard architecture of Inception-V2 using the Net2WiderNet algorithm\n",
    "\n",
    "We then apply the Net2WiderNet algorithm to the narrower model in order to recover the standard architecture of Inception-V2. The algorithm is applied to the Inception modules and the fully-connected layer only, since the rest of the network is already standard. The weights and biases of the student model (the wider one) are initialized with those of the teacher model (the narrower one), in such a way that the output of the student model is the same as the output of the teacher model for the same input at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Widening operation:  inception1.1\n",
      "Widening operation:  inception1.2\n",
      "Widening operation:  inception1.3\n",
      "Widening operation:  inception2.1\n",
      "Widening operation:  inception2.2\n",
      "Widening operation:  inception2.3\n",
      "Widening operation:  inception3.1\n",
      "Widening operation:  inception3.2\n",
      "Widening operation:  inception3.3\n",
      "Widening operation:  inception4.1\n",
      "Widening operation:  inception4.2\n",
      "Widening operation:  inception4.3\n",
      "Widening operation:  inception5.1\n",
      "Widening operation:  inception5.2\n",
      "Widening operation:  inception5.3\n",
      "Widening operation:  inception6.1\n",
      "Widening operation:  inception6.2\n",
      "Widening operation:  inception6.3\n",
      "Widening operation:  inception7.1\n",
      "Widening operation:  inception7.2\n",
      "Widening operation:  inception7.3\n",
      "Widening operation:  inception8.1\n",
      "Widening operation:  inception8.2\n",
      "Widening operation:  inception8.3\n",
      "Widening operation:  inception9.1\n",
      "Widening operation:  inception9.2\n",
      "Widening operation:  inception9.3\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Net2Net object from a (pre-trained) model\n",
    "net2net = net2net.net2net_wider.Net2Net(teacher_network=model)\n",
    "\n",
    "# Get the list of widening operations\n",
    "wider_operations = params.inceptionv2_imagenet.wider_operations\n",
    "\n",
    "# Add some noise to the copied weights (optional)\n",
    "sigma = 0.  # Standard deviation of the noise\n",
    "\n",
    "\n",
    "# Go through the list of widening operations\n",
    "for key in wider_operations.keys():\n",
    "\n",
    "    print(\"Widening operation: \", key)\n",
    "    \n",
    "    # Get the parameters of the wider operation\n",
    "    target_conv_layers = wider_operations[key][\"target_conv_layers\"]\n",
    "    next_layer = wider_operations[key][\"next_layers\"]\n",
    "    new_width = wider_operations[key][\"width\"]\n",
    "    batch_norm_layers = wider_operations[key][\"batch_norm_layers\"]\n",
    "\n",
    "    # Widen a layer of the network\n",
    "    net2net.net2wider(target_conv_layers=target_conv_layers,\n",
    "                      next_layers=next_layer,\n",
    "                      width=new_width,\n",
    "                      batch_norm_layers=batch_norm_layers,\n",
    "                      sigma=sigma)\n",
    "\n",
    "\n",
    "# Compute the output of the student network\n",
    "y_student = net2net.student_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check that the student and teacher models have the same output for the same input\n",
    "\n",
    "We check that the output of the student model is the same as the output of the teacher model for the same input at initialization. They can be slightly different if some noise has been added to the weights of the student model during the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher output:  tensor([[-0.2919, -0.1679, -0.1133,  0.3204,  0.0206,  0.0284,  0.1729,  0.2201,\n",
      "         -0.0654,  0.0823]], grad_fn=<AddmmBackward0>)\n",
      "Student output:  tensor([[-0.2919, -0.1679, -0.1133,  0.3204,  0.0206,  0.0284,  0.1729,  0.2201,\n",
      "         -0.0654,  0.0823]], grad_fn=<AddmmBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The outputs should be the same\n",
    "print(\"Teacher output: \", y_teacher)\n",
    "print(\"Student output: \", y_student, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Have a look at the student and teacher architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by displaying the architecture of the teacher model. We can check that the number of convolution channels at each layer within all Inception modules is reduced by a factor of $\\sqrt{0.3}$. The model has $1.886.577$ trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "GoogleNetBN                                   [1, 10]                   --\n",
       "├─Sequential: 1-1                             [1, 10]                   --\n",
       "│    └─Sequential: 2-1                        [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                       [1, 64, 112, 112]         9,472\n",
       "│    │    └─BatchNorm2d: 3-2                  [1, 64, 112, 112]         128\n",
       "│    │    └─ReLU: 3-3                         [1, 64, 112, 112]         --\n",
       "│    │    └─MaxPool2d: 3-4                    [1, 64, 56, 56]           --\n",
       "│    └─Sequential: 2-2                        [1, 192, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-5                       [1, 64, 56, 56]           4,160\n",
       "│    │    └─BatchNorm2d: 3-6                  [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-7                         [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-8                       [1, 192, 56, 56]          110,784\n",
       "│    │    └─BatchNorm2d: 3-9                  [1, 192, 56, 56]          384\n",
       "│    │    └─ReLU: 3-10                        [1, 192, 56, 56]          --\n",
       "│    │    └─MaxPool2d: 3-11                   [1, 192, 28, 28]          --\n",
       "│    └─Sequential: 2-3                        [1, 262, 14, 14]          --\n",
       "│    │    └─InceptionBN: 3-12                 [1, 139, 28, 28]          58,261\n",
       "│    │    └─InceptionBN: 3-13                 [1, 262, 28, 28]          115,985\n",
       "│    │    └─MaxPool2d: 3-14                   [1, 262, 14, 14]          --\n",
       "│    └─Sequential: 2-4                        [1, 455, 7, 7]            --\n",
       "│    │    └─InceptionBN: 3-15                 [1, 279, 14, 14]          111,501\n",
       "│    │    └─InceptionBN: 3-16                 [1, 279, 14, 14]          134,096\n",
       "│    │    └─InceptionBN: 3-17                 [1, 280, 14, 14]          153,116\n",
       "│    │    └─InceptionBN: 3-18                 [1, 288, 14, 14]          179,718\n",
       "│    │    └─InceptionBN: 3-19                 [1, 455, 14, 14]          258,884\n",
       "│    │    └─MaxPool2d: 3-20                   [1, 455, 7, 7]            --\n",
       "│    └─Sequential: 2-5                        [1, 560]                  --\n",
       "│    │    └─InceptionBN: 3-21                 [1, 455, 7, 7]            311,322\n",
       "│    │    └─InceptionBN: 3-22                 [1, 560, 7, 7]            433,028\n",
       "│    │    └─AdaptiveAvgPool2d: 3-23           [1, 560, 1, 1]            --\n",
       "│    │    └─Flatten: 3-24                     [1, 560]                  --\n",
       "│    └─Linear: 2-6                            [1, 10]                   5,610\n",
       "===============================================================================================\n",
       "Total params: 1,886,577\n",
       "Trainable params: 1,886,577\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 814.73\n",
       "===============================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.81\n",
       "Params size (MB): 7.55\n",
       "Estimated Total Size (MB): 47.96\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the architecture of the student network\n",
    "torchinfo.summary(model, input_size=(1,\n",
    "                                     params.inceptionv2_imagenet.NB_CHANNELS,\n",
    "                                     *params.inceptionv2_imagenet.IMAGE_SHAPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then display the architecture of the student model. We can check that the number of convolution channels at each layer within all Inception modules is the same as the standard model. The model has $5.998.362$ trainable parameters. Thus, the number of parameters in the teacher model is about $31.5\\%$ of the number of parameters in the student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "GoogleNetBN                                   [1, 10]                   --\n",
       "├─Sequential: 1-1                             [1, 10]                   --\n",
       "│    └─Sequential: 2-1                        [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                       [1, 64, 112, 112]         9,472\n",
       "│    │    └─BatchNorm2d: 3-2                  [1, 64, 112, 112]         128\n",
       "│    │    └─ReLU: 3-3                         [1, 64, 112, 112]         --\n",
       "│    │    └─MaxPool2d: 3-4                    [1, 64, 56, 56]           --\n",
       "│    └─Sequential: 2-2                        [1, 192, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-5                       [1, 64, 56, 56]           4,160\n",
       "│    │    └─BatchNorm2d: 3-6                  [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-7                         [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-8                       [1, 192, 56, 56]          110,784\n",
       "│    │    └─BatchNorm2d: 3-9                  [1, 192, 56, 56]          384\n",
       "│    │    └─ReLU: 3-10                        [1, 192, 56, 56]          --\n",
       "│    │    └─MaxPool2d: 3-11                   [1, 192, 28, 28]          --\n",
       "│    └─Sequential: 2-3                        [1, 480, 14, 14]          --\n",
       "│    │    └─InceptionBN: 3-12                 [1, 256, 28, 28]          164,432\n",
       "│    │    └─InceptionBN: 3-13                 [1, 480, 28, 28]          390,016\n",
       "│    │    └─MaxPool2d: 3-14                   [1, 480, 14, 14]          --\n",
       "│    └─Sequential: 2-4                        [1, 832, 7, 7]            --\n",
       "│    │    └─InceptionBN: 3-15                 [1, 512, 14, 14]          377,424\n",
       "│    │    └─InceptionBN: 3-16                 [1, 512, 14, 14]          450,456\n",
       "│    │    └─InceptionBN: 3-17                 [1, 512, 14, 14]          511,432\n",
       "│    │    └─InceptionBN: 3-18                 [1, 528, 14, 14]          606,784\n",
       "│    │    └─InceptionBN: 3-19                 [1, 832, 14, 14]          870,400\n",
       "│    │    └─MaxPool2d: 3-20                   [1, 832, 7, 7]            --\n",
       "│    └─Sequential: 2-5                        [1, 1024]                 --\n",
       "│    │    └─InceptionBN: 3-21                 [1, 832, 7, 7]            1,045,504\n",
       "│    │    └─InceptionBN: 3-22                 [1, 1024, 7, 7]           1,446,608\n",
       "│    │    └─AdaptiveAvgPool2d: 3-23           [1, 1024, 1, 1]           --\n",
       "│    │    └─Flatten: 3-24                     [1, 1024]                 --\n",
       "│    └─Linear: 2-6                            [1, 10]                   10,250\n",
       "===============================================================================================\n",
       "Total params: 5,998,362\n",
       "Trainable params: 5,998,362\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.58\n",
       "===============================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 51.62\n",
       "Params size (MB): 23.99\n",
       "Estimated Total Size (MB): 76.21\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the architecture of the student network\n",
    "torchinfo.summary(net2net.student_network, input_size=(1,\n",
    "                                                       params.inceptionv2_imagenet.NB_CHANNELS,\n",
    "                                                       *params.inceptionv2_imagenet.IMAGE_SHAPE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
