{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Import custom modules and packages\n",
    "import params.lenet_mnist\n",
    "import utils.log\n",
    "import utils.train\n",
    "import utils.test\n",
    "import utils.validate\n",
    "from lenet import LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_PARAMS = params.lenet_mnist.LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose several transforms together to be applied to data\n",
    "# (Note that transforms are not applied yet)\n",
    "transform = transforms.Compose([\n",
    "    # Modify the size of the images\n",
    "    transforms.Resize(params.lenet_mnist.IMAGE_SHAPE),\n",
    "    \n",
    "    # Convert a PIL Image or numpy.ndarray to tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize a tensor image with pre-computed mean and standard deviation\n",
    "    # (based on the data used to train the model(s))\n",
    "    # (be careful, it only works on torch.*Tensor)\n",
    "    transforms.Normalize(**params.lenet_mnist.NORMALIZE_PARAMS),\n",
    "])\n",
    "\n",
    "# Load the train dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = '.',\n",
    "    train = True,\n",
    "    transform = transform,\n",
    "    download = True,\n",
    ")\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root = '.',\n",
    "    train = False,\n",
    "    transform = transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "\n",
    "# As MNIST does not provide a validation dataset, we will split the train\n",
    "# dataset into a train and a validation dataset\n",
    "\n",
    "# Start by loading the train dataset, with the same transform as the\n",
    "# test dataset\n",
    "val_dataset = torchvision.datasets.MNIST(\n",
    "    root = '.',\n",
    "    train = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "# Set the train dataset size as a percentage of the original train dataset\n",
    "train_size = (len(train_dataset) - len(test_dataset))/len(train_dataset)\n",
    "\n",
    "# Splits train data indices into train and validation data indices\n",
    "train_indices, val_indices = train_test_split(range(len(train_dataset)),\n",
    "                                              train_size=train_size)\n",
    "\n",
    "# Extract the corresponding subsets of the train dataset\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(val_dataset, val_indices)\n",
    "\n",
    "\n",
    "# Combine a dataset and a sampler, and provide an iterable over the dataset\n",
    "# (setting shuffle argument to True calls a RandomSampler, and avoids to\n",
    "# have to create a Sampler object)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = LEARNING_PARAMS['batch_size'],\n",
    "    shuffle = True,\n",
    "    num_workers=12,  # Asynchronous data loading and augmentation\n",
    "    pin_memory=True,  # Increase the transferring speed of the data to the GPU\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = LEARNING_PARAMS['batch_size'],\n",
    "    shuffle = False,  # SequentialSampler\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=LEARNING_PARAMS[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples per split\n",
    "NB_TRAIN = len(train_dataset)\n",
    "NB_VAL = len(val_dataset)\n",
    "NB_TEST = len(test_dataset)\n",
    "\n",
    "# Display the splits ratio\n",
    "NB_SAMPLES = NB_TRAIN + NB_VAL + NB_TEST\n",
    "\n",
    "print(f\"{np.round(NB_TRAIN/NB_SAMPLES*100, 1)} % of the data for training ({NB_TRAIN} samples)\")\n",
    "print(f\"{np.round(NB_VAL/NB_SAMPLES*100, 1)} % of the data for validation ({NB_VAL} samples)\")\n",
    "print(f\"{np.round(NB_TEST/NB_SAMPLES*100, 1)} % of the data for testing ({NB_TEST} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model = LeNet(nb_classes=params.lenet_mnist.NB_CLASSES).to(device=device)\n",
    "\n",
    "# Define the loss function (combines nn.LogSoftmax() and nn.NLLLoss())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_PARAMS[\"learning_rate\"],\n",
    "    weight_decay=LEARNING_PARAMS[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors to store the loss and accuracy values\n",
    "loss_values = torch.zeros(2, LEARNING_PARAMS[\"nb_epochs\"])\n",
    "accuracy_values = torch.zeros(2, LEARNING_PARAMS[\"nb_epochs\"])\n",
    "\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(LEARNING_PARAMS[\"nb_epochs\"]):\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_accuracy = utils.train.train(model,\n",
    "                                                   device,\n",
    "                                                   train_loader,\n",
    "                                                   optimizer,\n",
    "                                                   criterion,\n",
    "                                                   epoch)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_accuracy = utils.validate.validate(model,\n",
    "                                                     device,\n",
    "                                                     val_loader,\n",
    "                                                     criterion,\n",
    "                                                     epoch) \n",
    "    \n",
    "    \n",
    "    print(\"Train accuracy: \", train_accuracy)\n",
    "    print(\"Validation accuracy: \", val_accuracy)\n",
    "    \n",
    "    # Store the computed losses\n",
    "    loss_values[0, epoch] = train_loss\n",
    "    loss_values[1, epoch] = val_loss\n",
    "    # Store the computed accuracies\n",
    "    accuracy_values[0, epoch] = train_accuracy\n",
    "    accuracy_values[1, epoch] = val_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "train_losses = loss_values[0]\n",
    "val_losses = loss_values[1]\n",
    "\n",
    "plt.plot(train_losses, \"b\", label=\"train loss\")\n",
    "plt.plot(val_losses, \"r\", label=\"validation loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "train_accuracies = accuracy_values[0]\n",
    "val_accuracies = accuracy_values[1]\n",
    "\n",
    "plt.plot(train_accuracies, \"b\", label=\"train accuracy\")\n",
    "plt.plot(val_accuracies, \"r\", label=\"validation accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_loss, test_accuracy = utils.test.test(model,\n",
    "                                           device,\n",
    "                                           test_loader,\n",
    "                                           criterion)\n",
    "\n",
    "print(\"Test loss: \", test_loss)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learning parameters table\n",
    "params_table = utils.log.parameters_table(\n",
    "    dataset=\"MNIST\",\n",
    "    learning_params=LEARNING_PARAMS)\n",
    "\n",
    "# Set the path to the results directory\n",
    "results_directory = \"logs/.\" +\\\n",
    "    datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "                        \n",
    "# Generate the log directory\n",
    "utils.log.generate_log(\n",
    "    results_directory=results_directory,\n",
    "    test_accuracy=test_accuracy,\n",
    "    parameters_table=params_table,\n",
    "    model=model,\n",
    "    accuracy_values=accuracy_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
